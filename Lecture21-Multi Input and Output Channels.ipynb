{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e81e2cc",
   "metadata": {},
   "source": [
    "## 多通道输入\n",
    "**每一个通道都有一个卷积核，结果是所有通道卷积结果的和->结果是单通道**\n",
    "![softmax-description](./imgs/21-1.png)\n",
    "公式如下：\n",
    "这里输入和卷积核的channel是相同的\n",
    "![softmax-description](./imgs/21-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b80bc",
   "metadata": {},
   "source": [
    "## 多通道输出\n",
    "目前，无论有多少输入通道，到目前为止我们都只能得到单输出通道\n",
    "\n",
    "我们可以有多个三维卷积核，每个卷积核生成一个输出通道。\n",
    "![softmax-description](./imgs/21-3.png)\n",
    "\n",
    "为什么要采用多通道输出？\n",
    "- 每一个输出通道可以识别特定模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae34f39",
   "metadata": {},
   "source": [
    "## 1 * 1卷积层\n",
    "- 它不识别空间模式，只是融合通道。\n",
    "![softmax-description](./imgs/21-4.png)\n",
    "- 1 * 1的卷积等价为一个全连接层\n",
    "- 之前没有认真理解过全连接层，这里记一些笔记：\n",
    "- 多层感知机的每一个hidden layer都是一个全连接层\n",
    "- 这里其实还是不太懂，等我复习的时候再去理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b13ca8",
   "metadata": {},
   "source": [
    "## 二维卷积层\n",
    "关于bias: 每一个输入通道和每一个输出通道都有一个偏差。\n",
    "\n",
    "10层，1M样本，10PFlop（而且只是前向）\n",
    "![softmax-description](./imgs/21-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa3cef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import d2l\n",
    "import torch.nn as nn\n",
    "\n",
    "def corr2d_multi_in(X, K):\n",
    "    # 先遍历 “X” 和 “K” 的第0个维度（通道维度），再把它们加在一起\n",
    "    # zip: 拆分第一个>1的维度，拆成维度为1并打包（zip有进行降维）\n",
    "    # 比如这里： X: 2, 3, 3, K: 2, 2, 2\n",
    "    # 那么x:3, 3, k: 2, 2\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7df386aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3]),\n",
       " torch.Size([2, 2, 2]),\n",
       " tensor([[ 56.,  72.],\n",
       "         [104., 120.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "# 进行多输入通道卷积\n",
    "'''\n",
    "# 这一句可以查看zip的尺寸\n",
    "for x, k in zip(X, K):\n",
    "    print(x.size(),k.size())\n",
    "    break\n",
    "'''\n",
    "X.size(),K.size(),corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a29c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。\n",
    "    # 最后将所有结果都叠加在一起\n",
    "    # 注意：第0维默认是output channel的维度\n",
    "    # c_o *  c_i * h * w\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfe5a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.stack((K, K + 1, K), 0) #拓展第0维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd65f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1.],\n",
      "         [2., 3.]],\n",
      "\n",
      "        [[1., 2.],\n",
      "         [3., 4.]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in K: # for k in K: 拆第0维\n",
    "    print(k)\n",
    "    print(k.size())\n",
    "    break\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2901c2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 56.,  72.],\n",
       "         [104., 120.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be546582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里为了证明： 1 * 1的卷积 等价于一个全连接层\n",
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape # \n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    # 全连接层中的矩阵乘法\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99a903a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3)) # in-channel = 3\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1)) # out-channel = 2, in-channel = 3\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6 #表达式为false时触发异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a169fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output channel, input channel\n",
    "conv2d = nn.Conv2d(1,1,kernel_size = 3, padding =1, stride = 2)\n",
    "conv2d = nn.Conv2d(1,1,kernel_size = (3,5), padding =(0,1), stride = (2,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
