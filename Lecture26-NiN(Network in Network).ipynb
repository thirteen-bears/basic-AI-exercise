{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac349f5",
   "metadata": {},
   "source": [
    "## 全连接层的问题\n",
    "- 卷积层需要较少的参数：$ c_i*c_o*k^2$\n",
    "- 全连接层需要的参数很多：我们来看卷积层后的第一个全连接层\n",
    "- LeNet: $16*5*5*120 = 48k$\n",
    "- AlexNet: $256*5*5*4096 = 26M$\n",
    "- VGG: $512*7*7*4096 = 102M$\n",
    "- 缺点：参数多就容易过拟合，所以需要做大量的正则化\n",
    "\n",
    "## 如何解决这个问题？\n",
    "\n",
    "1. NiN的思想: 用卷积层去替代全连接层\n",
    "![softmax-description](./imgs/26-1.png)\n",
    "\n",
    "2. NiN块\n",
    "- 一个卷积层后跟两个1 * 1的卷积层\n",
    "- 这里用1 * 1的卷积层，stride为1，no padding,所以输出形状和卷积层输出一样\n",
    "- 起到全连接层的作用\n",
    "\n",
    "3. NiN架构\n",
    "- 无全连接层\n",
    "- 交替使用NiN块和步幅为2的最大池化层：逐步减小高宽和增大通道数\n",
    "- 最后使用全局平均池化层得到输出：其输入通道数是类别数\n",
    "![softmax-description](./imgs/26-2.svg)\n",
    "\n",
    "## 总结\n",
    "- NiN使用卷积层增加了两个1 * 1的卷积层：后者对每个像素增加了非线性（加了relu激活函数）\n",
    "- NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层：不容易过拟合+ 更少的参数个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f381fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c48871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现NiN块\n",
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), \n",
    "        nn.ReLU(), \n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), \n",
    "        nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b7387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现NiN的模型：基于AlexNet架构的，所以可以对比他们区别\n",
    "net = nn.Sequential(\n",
    "    # 96就是对比AlexNet的\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n",
    "    nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cedb66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 我们创建一个数据样本来查看每个块的输出形状。\n",
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huhan/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
