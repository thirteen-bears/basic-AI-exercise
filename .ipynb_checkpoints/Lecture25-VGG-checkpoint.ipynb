{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac28925",
   "metadata": {},
   "source": [
    "###  怎样更好地更深，更大？\n",
    "- 提问:AlexNet比LeNet更深更大，怎样更好地更深，更大？\n",
    "- 选项：\n",
    "1. 更多的全连接层（太贵：大+计算量大）\n",
    "2. 更多的卷积层（在AlexNet的基础上加）\n",
    "3. 将卷积层组合成块->VGG!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd27f9",
   "metadata": {},
   "source": [
    "## VGG的idea\n",
    "- 实现了VGG块\n",
    "- 实验表明：同样计算开销的情况下，对更多的3 * 3 比堆少一点的5 * 5效果要好\n",
    "- VGG块： 3 * 3卷积（pad = 1）(n层，m通道), 2 * 2最大池化层（stride = 2）\n",
    "![softmax-description](./imgs/25-1.png)\n",
    "\n",
    "## VGG的架构\n",
    "- 多个VGG块后连接全连接层\n",
    "- VGG可以理解为更大更深的VGG\n",
    "![softmax-description](./imgs/25-3.png)\n",
    "\n",
    "## 总结\n",
    "- VGG使用可重复的卷积快来构建深度卷积神经网络\n",
    "- 使用不同卷积块个数和超参数可以得到不同复杂度的变种\n",
    "- VGG-11 使用可复用的卷积块构造网络。不同的 VGG 模型可通过每个块中卷积层数量和输出通道数量的差异来定义。块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。\n",
    "- 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fee38f",
   "metadata": {},
   "source": [
    "### 卷积输出层的尺寸公式\n",
    "假设：输入图片（Input）大小为I * I，卷积核（Filter）大小为K * K，步长（stride）为S，填充（Padding）的像素数为P，那卷积层输出（Output）的特征图大小为多少呢?\n",
    "\n",
    "可以得出推导公式：\n",
    "\n",
    "$O=((I-K+2P)/S)+1$\n",
    "\n",
    "### 最大池化层的输出尺寸\n",
    "$outwidth = （inwidth+2 * pad-kernelsize）/stride+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f302a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04897ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造vgg块\n",
    "# 先创建空列表，然后把layer放进列表，最后放到nn.Sequential里面去\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        # 这里输入和输出的尺寸是相同的 (i-3+2)+1\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels,\n",
    "                                kernel_size=3, padding=1))\n",
    "        \n",
    "        layers.append(nn.ReLU())\n",
    "        # 在num_convs中，就第一个卷积层改变通道数，后续卷积层都不改变通道数\n",
    "        in_channels = out_channels \n",
    "    # 通过最大池化层来减小feature map的长宽\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d19f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造vgg网络\n",
    "# 224/2^5 = 7，7就除不动了，所以是5块\n",
    "# 经典的思想：把网络分成5块，每一块高宽减半（在maxpooling时去减半），通道数翻倍\n",
    "# （num_convs, out_channels）\n",
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef6908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG11的结构\n",
    "def vgg(conv_arch):\n",
    "    conv_blks = []\n",
    "    in_channels = 1\n",
    "    # 卷积层部分\n",
    "    # 下一层的in_channels就是上一层的out_channels\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))\n",
    "        in_channels = out_channels\n",
    "\n",
    "    return nn.Sequential(\n",
    "        *conv_blks, # 卷积块的组合\n",
    "        nn.Flatten(),\n",
    "        # 全连接层部分\n",
    "        # 7是自己计算得到的\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10))\n",
    "net = vgg(conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huhan/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 224, 224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ef4efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_arch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-615766d517ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 因为vgg11比AlexNet计算量更大，我们构建了一个通道数较少的网络\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msmall_conv_arch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_arch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_conv_arch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_arch' is not defined"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 因为vgg11比AlexNet计算量更大，我们构建了一个通道数较少的网络\n",
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ce1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr会增大一些（随便调的）\n",
    "lr, num_epochs, batch_size = 0.05, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
