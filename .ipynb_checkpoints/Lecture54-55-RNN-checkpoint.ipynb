{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b48a60c",
   "metadata": {},
   "source": [
    "## 循环神经网络（RNN）\n",
    "\n",
    "#### Sidenote\n",
    "- Hidden是现实中存在，但是我们没观察到\n",
    "- Latent是现实中不存在的，可和hidden互换\n",
    "\n",
    "### 潜变量自回归模型\n",
    "- 潜变量h用于总结过去的信息\n",
    "![](./imgs/54-7.png)\n",
    "\n",
    "### 循环神经网络\n",
    "- RNN与MLP的区别是: 用额外的$W_{hh}$来存时序信息\n",
    "- $h_t$用的是$x_{t-1}$的内容,而不是$x_{t}$的内容\n",
    "- 计算损失的时候是比较$o_t$和$x_{t}$去计算损失\n",
    "- $\\fai$是激活函数\n",
    "![](./imgs/54-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6259a5",
   "metadata": {},
   "source": [
    "### 困惑度(perplexity)\n",
    "- 衡量一个语言模型的好坏可以用平均交叉熵\n",
    "- n是字典长度，$x_t$是真实词，p是语言模型的预测概率\n",
    "![](./imgs/54-8.png)\n",
    "- 历史原因NLP使用困惑度$exp(\\pi)$来衡量(平均交叉熵取指数),表示每一次的可能选项\n",
    "- 1表示完美，无穷是最大的情况\n",
    "\n",
    "### 梯度剪裁（Gradient Clip）\n",
    "- 在迭代中，计算这T个时间步上的梯度，在反向传播过程中，会产生长度为O(T)的矩阵乘法链，会导致数值不稳定\n",
    "- 梯度剪裁能有效预防梯度爆炸\n",
    "- 下式中的g表示所有层的梯度，全部放在一个向量里\n",
    "- 这个式子表达的意思是，假设g的长度大于theta，则投影回theta\n",
    "![](./imgs/54-5.png)\n",
    "\n",
    "### 更多RNN的应用\n",
    "![](./imgs/54-6.png)\n",
    "\n",
    "### 总结\n",
    "- RNN的输出取决于当前输入和前一时间的隐变量\n",
    "- 使用perplexity来衡量语言模型的好坏\n",
    "- 应用到语言模型中，RNN根据当前时刻词预测下一时刻词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7df4a0",
   "metadata": {},
   "source": [
    "## RNN从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95412a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab3977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_steps每一次看多长的序列\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "# vocal的用处是整数和词的转化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32764f59",
   "metadata": {},
   "source": [
    "### One-hot 编码介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b480858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index -> one-hot\n",
    "F.one_hot(torch.tensor([0, 2]), len(vocab)) # [0, 2]\n",
    "# 第一个变量是label，第二个变量是number_of_class\n",
    "# 这样返回基于第一个变量的对应的one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7791c4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0]]]),\n",
       " tensor([[0, 5],\n",
       "         [1, 6],\n",
       "         [2, 7],\n",
       "         [3, 8],\n",
       "         [4, 9]]),\n",
       " torch.Size([5, 2, 28]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对一个batch进行one-hot编码\n",
    "X = torch.arange(10).reshape((2, 5))  # 最后一个维度是时间\n",
    "# 这里需要做转置，X.T: [T,B,L]\n",
    "F.one_hot(X.T, 28),X.T,F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a272b9",
   "metadata": {},
   "source": [
    "### 初始化RNN的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ce4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    \n",
    "    num_inputs = num_outputs = vocab_size # 因为经过了onehot，所以输入长度等于vocab_size\n",
    "    \n",
    "    # 一个辅助函数: 初始化函数\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens)) # 挪掉这一行就是MLP\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    \n",
    "    # 给这些参数加入梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7babf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化隐藏状态: 因为0时刻没有隐藏状态，需要自己初始化\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    # 隐藏状态放在一个tuple里(之后会有多个隐藏状态)\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), ) # 也可以换成随机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d2ed0",
   "metadata": {},
   "source": [
    "### 在一个time-step内计算隐藏状态和输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5114eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # `inputs`的形状：(`time_step数量`，`批量大小`，`词表大小`)\n",
    "    # input里其实有time_step个样本\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state # state是一个tuple，里面只有一个H\n",
    "    outputs = []\n",
    "    # `X`的形状：(`批量大小`，`词表大小`)\n",
    "    for X in inputs: # 这里的X会沿着第0个维度去遍历，对time_step个样本进行遍历\n",
    "        H = torch.tanh(torch.mm(X, W_xh) \n",
    "                       + torch.mm(H, W_hh)  # 这一行的H是前一个时刻的隐藏状态\n",
    "                       + b_h) # \n",
    "        Y = torch.mm(H, W_hq) + b_q # 这里的H是当前时刻的隐藏状态\n",
    "        outputs.append(Y)  # outputs最后的尺寸为(time_step,batchsize)\n",
    "        # torch.cat(outputs, dim=0)在第0维去拼出来，列数还是vocab size，行数是batchsize*time_step\n",
    "    return torch.cat(outputs, dim=0), (H,) # 还要输出当前的隐藏状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351196aa",
   "metadata": {},
   "source": [
    "### 创建一个class来包装这些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eadb1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: #\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state): # 用call和用forward都可以进行前向传播\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    # 初始状态\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查输出是否正确\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在`prefix`后面生成新字符。\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测`num_preds`步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eafbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度。\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9397b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练模型一个迭代周期（定义见第8章）。\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和, 词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化`state`\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state`对于`nn.GRU`是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了`mean`函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）。\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aafe193",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ch8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-09ee4fca887b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n\u001b[0m\u001b[1;32m      2\u001b[0m           use_random_iter=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ch8' is not defined"
     ]
    }
   ],
   "source": [
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
    "          use_random_iter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
