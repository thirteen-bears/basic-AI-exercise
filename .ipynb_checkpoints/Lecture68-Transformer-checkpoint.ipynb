{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32c872e",
   "metadata": {},
   "source": [
    "## Transformer的架构\n",
    "- 也是基于encoder-decoder架构来处理序列对:encoder最后一层的输出传入decoder\n",
    "- 和使用注意力机制的seq2seq不同，transformer基础纯self-attention机制\n",
    "![](./imgs/68-4.png)\n",
    "- 对比seq2seq\n",
    "![](./imgs/68-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be75d8",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "- 对同一个key,value,query，希望抽取不同的信息；比如短距离关系和长距离关系\n",
    "- multi-head attention使用h个独立的注意力池化\n",
    "- 合并多个head的输出得到最终的输出\n",
    "![](./imgs/68-2.png)\n",
    "- 数学上的表示:\n",
    "![](./imgs/68-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8bccc",
   "metadata": {},
   "source": [
    "## 有掩码的Multi-head Attention\n",
    "- 解码器对序列中一个元素输出时，不应该概率该元素之后的元素（encoder中没关系的）\n",
    "- 也可以通过掩码来实现：计算$x_i$输出时，假装当前序列长度为i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13fb75",
   "metadata": {},
   "source": [
    "## 基于位置的前馈网络Positionwise FFN\n",
    "1. 步骤\n",
    "- 将输入形状由（b,n,d）变换成（bn,d）\n",
    "- 作用于两个全连接层\n",
    "- 输出形状由（bn,d）变换回（b,n,d）\n",
    "2. 理解\n",
    "- 等价于两层核窗口为1的一维卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2353e",
   "metadata": {},
   "source": [
    "## 层归一化 Layer normalization\n",
    "- batch normalization对每个channel/featuremap里的元素进行归一化\n",
    "- 不适合序列长度会变的NLP应用\n",
    "- 层归一化对每一个样本里的元素进行归一化\n",
    "- d维度是特征维度，b是batch维度，len是样本长度\n",
    "![](./imgs/68-6.png)\n",
    "![](./imgs/68-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b65b81",
   "metadata": {},
   "source": [
    "## 信息传递\n",
    "- 假设encoder中的输出为$y_1,...,y_n$\n",
    "- 将其作为decoder中的第i个transformer快中multi-head attention的key和value\n",
    "- 它的query来自于目标序列\n",
    "- 意味着encoder和decoder的块的个数和输出维度都是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a5327",
   "metadata": {},
   "source": [
    "## 预测\n",
    "- 预测第t+1个输出时\n",
    "- 解码器中输入前t个预测值\n",
    "- 在self-attention中，前t个预测值作为key和value,第t个预测值还作为query\n",
    "![](./imgs/68-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e37d08",
   "metadata": {},
   "source": [
    "## 总结\n",
    "- Transformer是一个纯使用注意力机制的encoder-decoder架构\n",
    "- Encoder和decoder都有n个transformer块\n",
    "- 每一个块里使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e2dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272a726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9125f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
